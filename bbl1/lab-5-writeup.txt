LING 567 Lab 5
Andrew Briand and Libbey Brown

We improved the following 3 phenomena in the choices file this week:
  Coordination
  Demonstrative Adjectives
  Embedded Clauses

1. Coordination

We repeat below our description of coordination from lab 3:

Coordination takes three forms in Tsova-Tush. First, in asyndetic coordination, clauses are placed next to each other without a conjunction. Second, clauses can be combined through the use of the coordinating conjunction, for example "e." Third, constituents and clauses can be combined through the use of a clitic. In the following example from our testsuite, "donkey" and "man" are coordinated with the use of the clitic "=e":

# 30: Coordination: Clitic
Source: author
Vetted: f
Judgment: g
Phenomena: Coordination
vir=e      st'ak=e         d-arst'-e
donkey=and man  =and cm(d/d)-gain.weight-prs
A donkey and a man are gaining weight

Note that since "donkey" and "man" have different genders, the verb takes d/d agreement. We therefore also added a negative test case to ensure this agreement is enforced:

# 31: Coordination: Clitic, subjects of different genders trigger (d/d) agreement
Source: author
Vetted: f
Judgment: u
Phenomena: Coordination
vir=e      st'ak=e   j-arst'-e
donkey=and man  =and cm(j/j)-gain.weight-prs
A donkey and a man are gaining weight

Additionally, here are examples of asyndetic and conjunction-based coordination:

k'uit' d-      arst'      -e,   st'ak v-      arst'      -e
cat    cm(d/d)-gain,weight-prs, man   cm(v/b)-gain.weight-prs
A cat is gaining weight and a man is gaining weight

k'uit' d-      arst'      -e,   e   st'ak v-      arst'      -e
cat    cm(d/d)-gain,weight-prs, and man   cm(v/b)-gain.weight-prs
A cat is gaining weight and a man is gaining weight

We added the following coordination strategies in our choices file to handle these cases:

 cs7_vp=on
  cs7_s=on
  cs7_pat=a
  cs8_n=on
  cs8_pat=omni
  cs8_mark=affix
  cs8_orth==e
  cs8_order=after
  cs9_vp=on
  cs9_s=on
  cs9_pat=mono
  cs9_mark=word
  cs9_orth=e
  cs9_order=before

We then attempted to add the following feature resolution patterns to handle agreement:

 fr1_name=noun-coordination
    fr1_feat1_name=gender
      fr1_feat1_rule1_left=same
      fr1_feat1_rule1_right=same
      fr1_feat1_rule1_par=same
      fr1_feat1_rule2_left=any
      fr1_feat1_rule2_right=nonmatching
      fr1_feat1_rule2_par=dd

However, this led to an error in loading the grammar which we discussed in class. We resolved the error as follows:

We added disjunctive types to the gender hierarchy:

non-dd := gender.
non-bd := gender.
non-jd := gender.
non-bb := gender.
non-jj := gender.
non-vb := gender.
non-bj := gender.
non-dj := gender.

dd := non-dj & non-jj & non-bj & non-bb & non-bd & non-jd & non-vb.
bj := non-dd & non-dj & non-jj & non-bb & non-bd & non-jd & non-vb.
dj := non-dd & non-jj & non-bj & non-bb & non-bd & non-jd & non-vb.
jd := non-dd & non-dj & non-jj & non-bj & non-bb & non-bd & non-vb.
vb := non-dd & non-dj & non-jj & non-bj & non-bb & non-bd & non-jd.
bb := non-dd & non-dj & non-jj & non-bj & non-bd & non-jd & non-vb.
bd := non-dd & non-dj & non-jj & non-bj & non-bb & non-jd & non-vb.
jj := non-dd & non-dj & non-bj & non-bb & non-bd & non-jd & non-vb.

We added rules to handle the different possible combinations of genders during coordination:

dd-nonmatching-dd-gend-coord-rule := coord-phrase &
  [ SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND dd,
    LCOORD-DTR.SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND dd,
    RCOORD-DTR.SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND non-dd ].

vb-nonmatching-dd-gend-coord-rule := coord-phrase &
  [ SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND dd,
    LCOORD-DTR.SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND vb,
    RCOORD-DTR.SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND non-vb ].

jj-nonmatching-dd-gend-coord-rule := coord-phrase &
  [ SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND dd,
    LCOORD-DTR.SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND jj,
    RCOORD-DTR.SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND non-jj ].

bj-nonmatching-dd-gend-coord-rule := coord-phrase &
  [ SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND dd,
    LCOORD-DTR.SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND bj,
    RCOORD-DTR.SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND non-bj ].

jd-nonmatching-dd-gend-coord-rule := coord-phrase &
  [ SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND dd,
    LCOORD-DTR.SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND jd,
    RCOORD-DTR.SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND non-jd ].

bb-nonmatching-dd-gend-coord-rule := coord-phrase &
  [ SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND dd,
    LCOORD-DTR.SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND bb,
    RCOORD-DTR.SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND non-bb ].

bd-nonmatching-dd-gend-coord-rule := coord-phrase &
  [ SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND dd,
    LCOORD-DTR.SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND bd,
    RCOORD-DTR.SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND non-bd ].

dj-nonmatching-dd-gend-coord-rule := coord-phrase &
  [ SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND dd,
    LCOORD-DTR.SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND dj,
    RCOORD-DTR.SYNSEM.LOCAL.CONT.HOOK.INDEX.PNG.GEND non-dj ].

We then added top and mid coord rules for these gender patterns:

n8-dd-nonmatching-dd-top-coord-rule := basic-n-top-coord-rule & omni-top-coord-rule & dd-nonmatching-dd-gend-coord-rule &
  [ SYNSEM.LOCAL.COORD-STRAT "8" ].

n8-dd-nonmatching-dd-mid-coord-rule := basic-n-mid-coord-rule & omni-mid-coord-rule & dd-nonmatching-dd-gend-coord-rule &
  [ SYNSEM.LOCAL.COORD-STRAT "8" ].

n8-vb-nonmatching-dd-top-coord-rule := basic-n-top-coord-rule & omni-top-coord-rule & vb-nonmatching-dd-gend-coord-rule &
  [ SYNSEM.LOCAL.COORD-STRAT "8" ].

n8-vb-nonmatching-dd-mid-coord-rule := basic-n-mid-coord-rule & omni-mid-coord-rule & vb-nonmatching-dd-gend-coord-rule &
  [ SYNSEM.LOCAL.COORD-STRAT "8" ].

n8-jj-nonmatching-dd-top-coord-rule := basic-n-top-coord-rule & omni-top-coord-rule & jj-nonmatching-dd-gend-coord-rule &
  [ SYNSEM.LOCAL.COORD-STRAT "8" ].

n8-jj-nonmatching-dd-mid-coord-rule := basic-n-mid-coord-rule & omni-mid-coord-rule & jj-nonmatching-dd-gend-coord-rule &
  [ SYNSEM.LOCAL.COORD-STRAT "8" ].

n8-bj-nonmatching-dd-top-coord-rule := basic-n-top-coord-rule & omni-top-coord-rule & bj-nonmatching-dd-gend-coord-rule &
  [ SYNSEM.LOCAL.COORD-STRAT "8" ].

n8-bj-nonmatching-dd-mid-coord-rule := basic-n-mid-coord-rule & omni-mid-coord-rule & bj-nonmatching-dd-gend-coord-rule &
  [ SYNSEM.LOCAL.COORD-STRAT "8" ].

n8-jd-nonmatching-dd-top-coord-rule := basic-n-top-coord-rule & omni-top-coord-rule & jd-nonmatching-dd-gend-coord-rule &
  [ SYNSEM.LOCAL.COORD-STRAT "8" ].

n8-jd-nonmatching-dd-mid-coord-rule := basic-n-mid-coord-rule & omni-mid-coord-rule & jd-nonmatching-dd-gend-coord-rule &
  [ SYNSEM.LOCAL.COORD-STRAT "8" ].

n8-bb-nonmatching-dd-top-coord-rule := basic-n-top-coord-rule & omni-top-coord-rule & bb-nonmatching-dd-gend-coord-rule &
  [ SYNSEM.LOCAL.COORD-STRAT "8" ].

n8-bb-nonmatching-dd-mid-coord-rule := basic-n-mid-coord-rule & omni-mid-coord-rule & bb-nonmatching-dd-gend-coord-rule &
  [ SYNSEM.LOCAL.COORD-STRAT "8" ].

n8-bd-nonmatching-dd-top-coord-rule := basic-n-top-coord-rule & omni-top-coord-rule & bd-nonmatching-dd-gend-coord-rule &
  [ SYNSEM.LOCAL.COORD-STRAT "8" ].

n8-bd-nonmatching-dd-mid-coord-rule := basic-n-mid-coord-rule & omni-mid-coord-rule & bd-nonmatching-dd-gend-coord-rule &
  [ SYNSEM.LOCAL.COORD-STRAT "8" ].

n8-dj-nonmatching-dd-top-coord-rule := basic-n-top-coord-rule & omni-top-coord-rule & dj-nonmatching-dd-gend-coord-rule &
  [ SYNSEM.LOCAL.COORD-STRAT "8" ].

n8-dj-nonmatching-dd-mid-coord-rule := basic-n-mid-coord-rule & omni-mid-coord-rule & dj-nonmatching-dd-gend-coord-rule &
  [ SYNSEM.LOCAL.COORD-STRAT "8" ].

Finally, we added the following to our rules.tdl:

n8-dd-nonmatching-dd-top-coord := n8-dd-nonmatching-dd-top-coord-rule.

n8-dd-nonmatching-dd-mid-coord := n8-dd-nonmatching-dd-mid-coord-rule.

n8-vb-nonmatching-dd-top-coord := n8-vb-nonmatching-dd-top-coord-rule.

n8-vb-nonmatching-dd-mid-coord := n8-vb-nonmatching-dd-mid-coord-rule.

n8-jj-nonmatching-dd-top-coord := n8-jj-nonmatching-dd-top-coord-rule.

n8-jj-nonmatching-dd-mid-coord := n8-jj-nonmatching-dd-mid-coord-rule.

n8-bj-nonmatching-dd-top-coord := n8-bj-nonmatching-dd-top-coord-rule.

n8-bj-nonmatching-dd-mid-coord := n8-bj-nonmatching-dd-mid-coord-rule.

n8-jd-nonmatching-dd-top-coord := n8-jd-nonmatching-dd-top-coord-rule.

n8-jd-nonmatching-dd-mid-coord := n8-jd-nonmatching-dd-mid-coord-rule.

n8-bb-nonmatching-dd-top-coord := n8-bb-nonmatching-dd-top-coord-rule.

n8-bb-nonmatching-dd-mid-coord := n8-bb-nonmatching-dd-mid-coord-rule.

n8-bd-nonmatching-dd-top-coord := n8-bd-nonmatching-dd-top-coord-rule.

n8-bd-nonmatching-dd-mid-coord := n8-bd-nonmatching-dd-mid-coord-rule.

n8-dj-nonmatching-dd-top-coord := n8-dj-nonmatching-dd-top-coord-rule.

n8-dj-nonmatching-dd-mid-coord := n8-dj-nonmatching-dd-mid-coord-rule.

2. Demonstrative adjectives

We repeat our description of demonstrative adjectives from lab 3:

There are three demonstrative adjectives in Tsova-Tush, each of which agrees with the noun in case. The following table reproduced from our reference shows their forms:

	'this'	'that'	'yon'
ABS	e	is	o
OBL	eq	icx	oq

The following are some of the examples from our test suite demonstrating the required agreement patterns:

# Demonstrative adjectives: absolutive
Source: author
Vetted: f
Judgment: g
Phenomena: Demonstrative adjectives
e        st'ak       v-arst'-e
this.ABS man   cm(v/b)-gain.weight-prs
This man is gaining weight

# Demonstrative adjectives: absolutive
Source: author
Vetted: f
Judgment: u
Phenomena: Demonstrative adjectives
eq       st'ak       v-arst'-e
this.OBL man   cm(v/b)-gain.weight-prs
This man is gaining weight

# 150: Demonstrative adjectives: oblique
Source: author
Vetted: f
Judgment: g
Phenomena: Demonstrative adjectives
icx      bader-i-v    ʡok'-i                          d-aq'-o
that.OBL child-PL-ERG crescent.shaped.pastry-PL cm(d/d)-eat-IMPF
Those children are eating pancakes

# 160: Demonstrative adjectives: oblique
Source: author
Vetted: f
Judgment: u
Phenomena: Demonstrative adjectives
is       bader-i-v    ʡok'-i                          d-aq'-o
that.ABS child-PL-ERG crescent.shaped.pastry-PL cm(d/d)-eat-IMPF
Those children are eating pancakes

After our lab 3 write-up it was asked why we consider these to be demostrative adjectives and not determiners. We chose to describe them as demonstrative adjectives simply because our reference describes them as such but says:

"I have observed that some speakers use the absolutive form of
demonstrative adjectives in all contexts, using the oblique form rarely or not at all. This pattern is consistent with the trajectory of the grammaticalization of adnominal demonstratives into determiners."

Our choices file already included some of these adjectives as determiners, namely 'e', 'equs', 'is', 'eq', but without case information. We added the case information as follows:

  det4_name=det4
    det4_stem1_orth=e
    det4_stem1_pred=_exist_q_rel
    det4_feat1_name=case
    det4_feat1_value=abs

  det6_name=det6
    det6_stem1_orth=equs
    det6_stem1_pred=_exist_q_rel
    det6_feat1_name=case
    det6_feat1_value=erg, gen, dat, all, con, instr, adverbial, ill, dir, loc, all-loc, ill-loc

  det8_name=det8
    det8_stem1_orth=is
    det8_stem1_pred=_exist_q_rel
    det8_feat1_name=case
    det8_feat1_value=abs

  det11_name=det11
    det11_stem1_orth=eq
    det11_stem1_pred=_exist_q_rel
    det11_feat1_name=case
    det11_feat1_value=erg, gen, dat, all, con, instr, adverbial, ill, dir, loc, all-loc, ill-loc

Then we created determiner types for 'icx', 'o', and 'oq' as follows:

  det19_name=det19
    det19_stem1_orth=icx
    det19_stem1_pred=_exist_q_rel
    det19_feat1_name=case
    det19_feat1_value=erg, gen, dat, all, con, instr, adverbial, ill, dir, loc, all-loc, ill-loc
  det20_name=det20
    det20_stem1_orth=o
    det20_stem1_pred=_exist_q_rel
    det20_feat1_name=case
    det20_feat1_value=abs
  det21_name=det21
    det21_stem1_orth=oq
    det21_stem1_pred=_exist_q_rel
    det21_feat1_name=case
    det21_feat1_value=erg, gen, dat, all, con, instr, adverbial, ill, dir, loc, all-loc, ill-loc

3. Embedded clauses (phenomena added to testsuite last week, updated in customization this week):

Our literature states there are thirteen different semantic categories of verbs that take embedded clauses as arguments. In general, verbs which take embedded finite clauses are marked with d/d agreement 
(if they are verbs that take agreement marking), though there are some one-off unique cases. To clarify based on the comments from last week's lab: transitive verbs would typically agree with the non-subject
argument, and so the embedded clause argument is treated as though it has d/d gender.

Based on the machine translation sentences, we changed the verbs we are focusing on for this phenomena.  The verbs we are focusing on are:
	ask, 'xat'' and 'xet'', from the "utterance" semantic class
	think, 'dak'lev', from the "propositional attitude" semantic class
	know, 'xeʔ', from the "knowledge and acquisition of knowledge" semantic class
None of these verbs are agreement-marking verbs.

There is no specific morphological marking on the verb to indicate its argument is an embedded clause.
There is no marking on the embedded clause.  Embedded clauses are not nominalized.

We did not see anything in our literature specifically defining the word order for a sentence containing an embedded clause, but every example we located in the corpus and in Hauk's literature had SVO word order, with the embedded clause occurring after the verb.  As we were unable to determine if a different word order would be ungrammatical, we chose to use the SVO word order for all our test sentences but not enforce word order for grammaticality judgments.

Embedded finite clauses in Tsova-Tush are introduced in one of three ways:
	1. The complementizer 'me'
	2. A content question word
	3. No complementizer

Clarification added based on last week's comments: to the best of our understanding, the question word appears both clause-initially and immediately preceding the verb.
That is, for an embedded question, the order for the embedded clause must be: question word - verb - everything else in the phrase.

Our literature states that the complementizer 'me' is used most often for non-finite embedded clauses, but we found many examples in our corpus that used 'me' to introduce finite embedded declarative clauses (in fact, our review of the corpus had more sentences introducing embedded finite clauses with the 
complementizer than without).  

There were fewer examples in our corpus of embedded interrogative clauses, but there were sentences included in Hauk showing that an embedded interrogative clause can occur with the complementizer 'me' in addition to a content question word, or simply occuring with the content question word. We were unbale to locate sentences that met the testsuite requirements for embedded yes-no questions - there were many examples with content questions, but no sentences in the corpus with yes-no questions.

It was challenging to generate ungrammatical test sentences for embedded clauses, as they can occur with or without the complementizer and we did not want to enforece
any grammaticality judgments regarding word order as we were uncertain if there is any requirement.  It seems likely that the complementizer 'me' is required to occur directly
before the embedded clause (based on the distribution we saw in the corpus) but as we weren't sure we did not want to use this as a judgment for grammaticality.

We first enabled clausal complements through the customization software.  We allowed the clausal complement to occur in the same position as regular noun complements, as this is the only option available for free word order languages.  We added an optional complementizer, spelled 'me', per the information obtained in our literature.  We created just one complementizing strategy, for declarative clauses and for questions, beacuse at this point there does not seem to be a clear difference between the two.  The issues we ran into last week enforcing word order for non-embedded question words prevents us from enforcing word orer for embedded questions as well.

section=clausal-comp
  comps1_clause-pos-same=on
  comps1_ques=prop
  comps1_comp=opt
    comps1_stem1_orth=me

We created new verb types for our clausal-complement-taking verbs.  We created different types for question-taking and declarative-taking verbs in case we determined a distinction between the two types 
down the road.  We were unable to locate the perfective-stem spelling for 'think' and 'know', so there is no imperfective stem for this verb type (this will only be an issue if we want to create future-tense 
versions of these verbs, and we do not think this will be necessary).  As we were able to determine both spellings for the verb 'ask', we created lexical entries for both the perfective and imperfective stems.

verb192_name=clause_comp_dec_impf
  verb192_valence=trans,comps1
    verb192_stem1_orth=dak'lev
    verb192_stem1_pred=_think_v_rel
    verb192_stem2_orth=xeʔ
    verb192_stem2_pred=_know_v_rel
  verb193_name=clause_comp_ques_impf
  verb193_valence=trans,comps1
    verb193_stem1_orth=xet'
    verb193_stem1_pred=_ask_v_rel
  verb194_name=clause_comp_ques_perf
  verb194_valence=trans,comps1
    verb194_stem1_orth=xat'
    verb194_stem1_pred=_ask_v_rel

The following sentences were added to our testsuite last week, with the exception of the last sentences, which we changed to better align with the machine translation exercise:

# 86: Embedded clauses: declarative
Source: b 4200
Vetted: s
Judgment: g
Phenomena: embedded clauses
malo dak'lev me , ǰemo-n y-ec' 
Malo think COMP , Jemo-DAT CMd/jsg-love
Malo thinks that Jemo loves her. 

# 87: Embedded clauses: declarative
Source: author
Vetted: f
Judgment: g
Phenomena: embedded clauses
malo dak'lev ǰemo-n y-ec' 
Malo think Jemo-DAT CMd/jsg-love
Malo thinks Jemo loves her. 

# 88: Embedded clauses: declarative
Source: b 4210
Vetted: s
Judgment: g
Phenomena: embedded clauses
malo dak'lev me , ǰemo v-ec' 
Malo think COMP , Jemo CMd/bsg-love
Malo thinks that she loves Jemo. 

# 89: Embedded clauses: declarative
Source: author
Vetted: f
Judgment: g
Phenomena: embedded clauses
malo dak'lev ǰemo v-ec' 
Malo think Jemo CMd/bsg-love
Malo thinks she loves Jemo. 

# 90: Embedded clauses: question
Source: author
Vetted: f
Judgment: g
Phenomena: embedded clauses
malo xat'-en vux tit'er
malo ask-AORIST what cut-AORIST
Malo asked what did (she/he) cut.


Sentence translation for MMT:

To begin the mmt process, we searched through our lexicon and the corpus for the necessary vocabulary words, and then found sentences
using the words in context to help with translation.  The verb 'chase' is actually a complicated verb in Tsova-Tush, as it starts with an 
intransitive verb meaning 'run' ('ac' or 'oc', spelling varies depending on the perfective/imperfective stems) and then has a valence-changing 
affix that changes it to a transitive verb that was translated in our corpus as 'chase' (ac'y--al/oc'y--al).  Based on recommendations from Canvas, 
we will simply treat this as a transitive verb rather than using a valence-changing morphology rule.  This verb also requires the use of a case
marker we haven't dealt with in great detail (contact case) so we will need to ensure this sentence parses appropriately.

We were able to translate the first five sentences, and two of the later wh-question sentences (who sleeps, what do the dogs chase). These
sentences are included in our testsuite and in our bbl.txt document, and are duplicated below:

# 91: Machine translation
Source: author
Vetted: f
Judgment: g
Phenomena: intransitive verb
pħu-i toħ
dog-PL sleep-PRES.UNINFLECTED
Dogs sleep

# 92: Machine translation
Source: author
Vetted: f
Judgment: g
Phenomena: agreement
pħu-i mankaⁿ-i-x b-ac'yal
dog(abs)-PL car-PL-CON AGR-chase.transitive-PRES.UNINFLECTED
Dogs chase cars

# 93: Machine translation
Source: author
Vetted: f
Judgment: g
Phenomena: agreement
as ħo-x v-ac'yal
I(abs) you-CON AGR-chase.transitive-PRES.UNINFLECTED
I chase you

# 94: Machine translation
Source: author
Vetted: f
Judgment: g
Phenomena: agreement
pħu-i-v b-aq'-o
dog-pl-erg AGR-eat-pres
Dogs eat

# 95: Machine translation
Source: author
Vetted: f
Judgment: g
Phenomena: negation
pħu-i mankaⁿ-i-x co b-ac'yal
dog(abs)-PL car-PL-CON not AGR-chase.transitive-PRES.UNINFLECTED
The dogs don't chase cars

# 96: Machine translation
Source: author
Vetted: f
Judgment: g
Phenomena: wh-questions
pħu-i st'e-x d-ac'yal
dog-PL what-CON AGR-chase.trans.PRES.UNINFLECTED
What do the dogs chase

# 97 Machine translation
Source: author
Vetted: f
Judgment: g
Phenomena: wh-questions
meⁿ toħ
who(abs) sleep.PRES.UNINFLECTED
Who sleeps

At this point, it seems unlikely that we will get to the sentences "The dog sleeps because the cat sleeps" or "The dog sleeps after the cat sleeps"
because those are not phenomena we have planned to explore.  Additionally, we have had issues finding enough documentation and examples in
the corpus for yes/no questions, especially embedded yes/no questions, so "I ask whether you know that dogs chase cars" may be challenging for
us as well.  We haven't looked into prepositions yet ("Dogs in the park...") and we haven't really explored adjectives either, but there
should hopefully be enough support in our corpus to translate those sentences.


MT set-up:

Setting up the MT was relatively smooth. We had no difficulties compiling our grammar with ace and we were able to follow the steps in the lab instructions all the way up until running ./translate-line.sh. At this point, the VM ran out of RAM without producing any output while trying to translate the first sentence from English to Tsova-Tush. Tsova-Tush to Tsova-Tush generation did not work because the sentence was not yet parsing using our grammar. This was because the original lexical entry for the verb sleep in our grammar was incorrect. We deleted it and added the following instead:

toħ := intrans_imperfective-verb-lex &
  [ STEM < "toħ" >,
    SYNSEM.LKEYS.KEYREL.PRED "_sleep_v_rel" ].

This verb is not split-S and does not agree with its subject in gender so we added a new verb type:

intrans_imperfective-verb-lex := intransitive-verb-lex & impf_stems-rule-dtr & verb-pc1-rule-dtr & verb-pc10-rule-dtr & verb-pc11-rule-dtr & verb-pc15-rule-dtr & verb-pc34-rule-dtr & verb-pc38-rule-dtr & verb-pc45-rule-dtr & verb-pc48-rule-dtr & verb-pc5-rule-dtr & verb-pc58-rule-dtr & verb-pc6-rule-dtr & verb-pc9-rule-dtr &
  [ SYNSEM.LOCAL.CAT.VAL.SUBJ.FIRST.LOCAL [ CAT.HEAD.CASE abs ],
    INFLECTED.IMPF_STEMS-FLAG - ].

After adding this lexical entry and rule, the first sentence parsed. Then, Tsova-Tush to Tsova-Tush generation produced 32,833 results. English to Tsova-Tush translation produced 32,846 results.


Performance:

Start of week performance on test corpus:

  How many items parsed? 144 out of 999 
  What is the average number of parses per parsed item? 35.42
  How many parses did the most ambiguous item receive? 726
  What sources of ambiguity can you identify? The main source of ambiguity appears to be bad coordination (e.g. ungrammatical asyndeton between nouns).

This profile can be found in tsdb/home/bbl/lab5_start/corpus.

Start of week performance on testsuite:

  How many items parsed? 25 out of 97 (63 of which are positive examples)
  What is the average number of parses per parsed item? 6.12
  How many parses did the most ambiguous item receive? 44
  What sources of ambiguity can you identify? Bad coordination as in the test corpus. Most sentences that do not have two nouns appearing next to each other have a reasonable amount of analyses (1-2).

This profile can be found in tsdb/home/bbl/lab5_start/lab5.

End of week performance on test corpus:

  How many items parsed? 128 out of 999
  What is the average number of parses per parsed item? 44.41
  How many parses did the most ambiguous item receive? 438
  What sources of ambiguity can you identify? Sentences like the following seem to be very ambiguous because they are interpreted by the grammar in some analyses as the asyndetic coordination of two sentences (something we added to the grammar this week):

so-n   co  qet , vux          aɬ-in              oqus
1S DAT NEG know, what (NOM.S) say-aorist.formant 3S.ERG 
I don't know what s/he said.

(The above was the worst at 438 parses).

This profile can be found in tsdb/home/bbl/lab5_end/corpus.

End of week performance on testsuite:

  How many items parsed? 39 out of 97 (63 of which are positive examples)
  What is the average number of parses per parsed item? 8.36
  How many parses did the most ambiguous item receive? 68
  What sources of ambiguity can you identify? The main source of ambiguity appears to be embedded clauses being interpreted as asyndetic coordination (as in the test corpus). Additionally, as we saw during class on thursday, in sentences like the following exhibiting coordination between nouns, there is increased ambiguity from N8-LEFT being the daughter of BARE-NP without coordinating with anything:

# 30: Coordination: Clitic
Source: author
Vetted: f
Judgment: g
Phenomena: Coordination
vir=e st'ak=e d-arst'-e
donkey=and man=and cm(d/d)-gain.weight-prs
A donkey and a man are gaining weight

This profile can be found in tsdb/home/bbl/lab5_end/lab5.

Examination of the semantics of 10 sentences (same sentences as from the end of lab 3):

1.
k'ač'aro dapxol meq'-o .
wool:ERG warmth stay present formant (+impf) .
Wool holds warmth.

This sentence no longer parses.

2.
bʕerc'-a-ɣ v-erc'-in st'ak' .
wolf SG thematic ending? ADV CM (M.sg) turn into (+preradical CM) aorist formant man, person, human .
The man became a wolf.

The semantics for this sentence are still reasonable but we still need to rename some of the predicates, like '_turn into (+preradical cm)_v_rel'.

3.
kortʷ-e-ɣ b-erc'-in qer .
head SG thematic vowel between stem and case ending ADV CM (B/d) turn into (+preradical CM) aorist formant stone .
A stone became the head.

The semantics for this sentence again look reasonable but we need to rename '_turn into (+preradical cm)_v_rel'.

4.
k'uit'i d-a .
cat CM (d/d) be (present) .
It's a cat.

This sentence no longer parses. We caused these sentence not to work when adding tense and aspect morphology and so will need to repair the agreement for this verb.

5.
k'uit'i d-a-nuic .
cat CM (d/d) be (present) evidential? indeed? .
It is indeed a cat.

This sentence also no longer parses.

6.
k'uit'i d-a=e .
cat CM (d/d) be (present) =and .
It's a cat.

This sentence also no longer parses.

7.
band y-a .
rag, duster CM (y/y) be (present) .
It's a rag.

This sentence also no longer parses.

8.
vir d-a-ra .
donkey CM (d/d) be (present) imperfect present (after present formant) . 
There was a donkey.

This sentence also no longer parses.

9.
moħ d-ec'e aɬ-an ?
how CM (d/d) should, must (+CM) say, talk about (pfv; no CM) infinitive ?
How should one say it?

'_should,must (+cm)_v_rel' should still simply be '_should,must_v_rel' here. Additionally, 'how' is still interpreted as a noun.

10.
kayrcxi d-itː-o .
clothing; the wash CM (d/d) wash (+preradical CM) present formant (+impf) .
They're washing clothes.

'_wash (+preradical cm)_v_rel' should still simply be '_wash_v_rel'. One of the MRS's is now completely missing an AVM for wash, and only has an 'exist_q_rel' and 'clothing;the wash_n_rel'. The two other analyses are as before, one having clothing correctly identified as the object of the verb and the other incorrectly as the subject of the verb.
